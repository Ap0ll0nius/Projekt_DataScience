{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7195b034-4f2b-4a60-a289-e482e0105092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pascal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pascal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Pascal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   helpfulVotes publishedDate publishedPlatform  rating  \\\n",
      "0             0    2024-10-04             OTHER       4   \n",
      "1             0    2024-09-17             OTHER       4   \n",
      "2             0    2024-09-16            MOBILE       5   \n",
      "3             0    2024-09-07             OTHER       4   \n",
      "4             0    2024-08-15            MOBILE       4   \n",
      "\n",
      "                                                text  \\\n",
      "0  Visit today university of oxford amazing archi...   \n",
      "1  Having some difficulty with walking and standi...   \n",
      "2  Founded in the 12th century, it’s not only one...   \n",
      "3  To visit this interesting museum it is essenti...   \n",
      "4  伦敦到牛津的高铁一般都是从两个站出发：\\n去程：London Paddington or L...   \n",
      "\n",
      "                                 title  tripType  \n",
      "0                                Happy    FAMILY  \n",
      "1  Fabulous Buildings and Architecture   COUPLES  \n",
      "2   One of world’s oldest universities  BUSINESS  \n",
      "3       Oxford University Press Museum      SOLO  \n",
      "4                                 牛津大学   FRIENDS  \n",
      "   10  100  1000  100s  1030  1069  1090s  1096  10th  10week  ...  yr  zero  \\\n",
      "0   0    0     0     0     0     0      0     0     0       0  ...   0     0   \n",
      "1   0    0     0     0     0     0      0     0     0       0  ...   0     0   \n",
      "2   0    0     0     0     0     0      0     0     0       0  ...   0     0   \n",
      "3   0    0     0     0     0     0      0     0     0       0  ...   0     0   \n",
      "4   0    0     0     0     0     0      0     0     0       0  ...   0     0   \n",
      "\n",
      "   伦敦到牛津的高铁一般都是从两个站出发  去程london  叹息桥  基督教堂太热门了一定要提前一周左右在google  好像包含讲解器  \\\n",
      "0                   0         0    0                         0        0   \n",
      "1                   0         0    0                         0        0   \n",
      "2                   0         0    0                         0        0   \n",
      "3                   0         0    0                         0        0   \n",
      "4                   1         1    1                         1        1   \n",
      "\n",
      "   门票15镑  门票8镑打卡了  非常好看的圆顶图书馆  \n",
      "0      0        0           0  \n",
      "1      0        0           0  \n",
      "2      0        0           0  \n",
      "3      0        0           0  \n",
      "4      1        1           1  \n",
      "\n",
      "[5 rows x 5434 columns]\n",
      "    10  100  1000  100s  1030  1069  1090s  1096  10th  10week  ...   yr  \\\n",
      "0  0.0  0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0     0.0  ...  0.0   \n",
      "1  0.0  0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0     0.0  ...  0.0   \n",
      "2  0.0  0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0     0.0  ...  0.0   \n",
      "3  0.0  0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0     0.0  ...  0.0   \n",
      "4  0.0  0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0     0.0  ...  0.0   \n",
      "\n",
      "   zero  伦敦到牛津的高铁一般都是从两个站出发  去程london       叹息桥  基督教堂太热门了一定要提前一周左右在google  \\\n",
      "0   0.0            0.000000  0.000000  0.000000                  0.000000   \n",
      "1   0.0            0.000000  0.000000  0.000000                  0.000000   \n",
      "2   0.0            0.000000  0.000000  0.000000                  0.000000   \n",
      "3   0.0            0.000000  0.000000  0.000000                  0.000000   \n",
      "4   0.0            0.140675  0.140675  0.140675                  0.140675   \n",
      "\n",
      "    好像包含讲解器     门票15镑   门票8镑打卡了  非常好看的圆顶图书馆  \n",
      "0  0.000000  0.000000  0.000000    0.000000  \n",
      "1  0.000000  0.000000  0.000000    0.000000  \n",
      "2  0.000000  0.000000  0.000000    0.000000  \n",
      "3  0.000000  0.000000  0.000000    0.000000  \n",
      "4  0.140675  0.140675  0.140675    0.140675  \n",
      "\n",
      "[5 rows x 5434 columns]\n",
      "Topic 1: place, around, see, history, buildings, tour, visit, colleges, university, oxford\n",
      "Topic 2: get, really, bus, day, interesting, us, took, walking, guide, tour\n",
      "Topic 3: magdalen, filmed, colleges, dining, harry, potter, christ, hall, church, college\n",
      "Topic 4: amazing, architecture, nice, potter, harry, beautiful, history, great, visit, place\n",
      "Topic 5: great, grounds, buildings, architecture, walking, city, lovely, beautiful, walk, around\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df = pd.read_csv(\"university_of_oxford_tripadvisor_reviews.csv\")\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "if 'text' not in df.columns:\n",
    "    print(\"Error: No column named 'text'. Check your dataset.\")\n",
    "else:\n",
    "    texts = df['text']\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(bow_df.head())\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf_df.head())\n",
    "\n",
    "\n",
    "n_topics = 5  # Number of topics to extract\n",
    "lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i, component in enumerate(lsa_model.components_):\n",
    "    top_terms = [terms[idx] for idx in component.argsort()[-10:]]  # Top 10 terms\n",
    "    print(f\"Topic {i+1}: {', '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d83b237-44b4-4e17-b4a5-8f5225943b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'texts' is your preprocessed text data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 6\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Apply LSA\u001b[39;00m\n\u001b[0;32m      9\u001b[0m n_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Adjust based on your dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Assuming 'texts' is your preprocessed text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# Apply LSA\n",
    "n_topics = 5  # Adjust based on your dataset\n",
    "lsa_model = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Extract top words for each topic\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "topics = []\n",
    "for i, component in enumerate(lsa_model.components_):\n",
    "    top_terms = [terms[idx] for idx in component.argsort()[-10:]]  # Top 10 words\n",
    "    topics.append(top_terms)\n",
    "    print(f\"Topic {i+1}: {', '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b78f7cd-52b5-42b7-b1b3-bb2219552080",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenized_texts \u001b[38;5;241m=\u001b[39m [word_tokenize(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a dictionary and corpus\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdictionary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_texts = [word_tokenize(doc) for doc in df['cleaned_text']]\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d1972c-3916-41d9-ad0c-6b5da1e72fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
